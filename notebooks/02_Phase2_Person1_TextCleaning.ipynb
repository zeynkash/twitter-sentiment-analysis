{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+nI5vo97js1xZ6IQYJej1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"OEXDbHYSZ6Q2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766238240245,"user_tz":-180,"elapsed":19949,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"40e0c862-28db-46ec-f22c-d4d8bbd00ab8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","‚úÖ Setup complete!\n","Current directory: /content/drive/MyDrive/TwitterSentimentProject\n"]}],"source":["# Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/MyDrive/TwitterSentimentProject')\n","\n","print(\"‚úÖ Setup complete!\")\n","print(\"Current directory:\", os.getcwd())"]},{"cell_type":"code","source":["# Import libraries\n","import pandas as pd\n","import numpy as np\n","import re\n","import string\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Download NLTK data\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","print(\"‚úÖ Libraries imported!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jX9S1D4cAG62","executionInfo":{"status":"ok","timestamp":1766238292573,"user_tz":-180,"elapsed":7068,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"f36db0db-27e7-4fff-bf06-cddfe966c022"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Libraries imported!\n"]}]},{"cell_type":"code","source":["# Load cleaned data from Phase 1\n","df = pd.read_csv('data/processed/tweets_cleaned.csv')\n","\n","print(f\"Dataset loaded: {len(df):,} rows\")\n","print(f\"Columns: {list(df.columns)}\")\n","print(f\"\\nSentiment distribution:\")\n","print(df['sentiment'].value_counts())\n","\n","# Show sample tweets\n","print(\"\\nSample tweets (before cleaning):\")\n","for i, text in enumerate(df['text'].head(3), 1):\n","    print(f\"{i}. {text}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8VmCuAXzAW1y","executionInfo":{"status":"ok","timestamp":1766238338709,"user_tz":-180,"elapsed":1890,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"2a645170-ffcf-4f85-811e-f1d300298e75"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset loaded: 99,357 rows\n","Columns: ['sentiment', 'id', 'date', 'query', 'user', 'text', 'tweet_length', 'word_count', 'hashtags', 'mentions', 'hashtag_count', 'mention_count', 'has_url']\n","\n","Sentiment distribution:\n","sentiment\n","1    49739\n","0    49618\n","Name: count, dtype: int64\n","\n","Sample tweets (before cleaning):\n","1. @stargazer60 that's awesome \n","\n","2. @cunningstunts22  yes... i am!  \n","\n","3. N my bed aaallll alone \n","\n"]}]},{"cell_type":"code","source":["# Text cleaning functions\n","\n","def remove_urls(text):\n","    \"\"\"Remove URLs from text\"\"\"\n","    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","\n","def remove_mentions(text):\n","    \"\"\"Remove @mentions\"\"\"\n","    return re.sub(r'@\\w+', '', text)\n","\n","def remove_hashtags(text):\n","    \"\"\"Remove # symbol but keep the word\"\"\"\n","    return re.sub(r'#', '', text)\n","\n","def remove_numbers(text):\n","    \"\"\"Remove numbers\"\"\"\n","    return re.sub(r'\\d+', '', text)\n","\n","def remove_punctuation(text):\n","    \"\"\"Remove punctuation\"\"\"\n","    return text.translate(str.maketrans('', '', string.punctuation))\n","\n","def remove_extra_spaces(text):\n","    \"\"\"Remove extra whitespaces\"\"\"\n","    return ' '.join(text.split())\n","\n","def to_lowercase(text):\n","    \"\"\"Convert to lowercase\"\"\"\n","    return text.lower()\n","\n","print(\"‚úÖ Cleaning functions created!\")\n","\n","# Test functions\n","sample_text = \"@john Hey! Check this out https://example.com #awesome 123\"\n","print(\"\\nOriginal:\", sample_text)\n","print(\"After URL removal:\", remove_urls(sample_text))\n","print(\"After mention removal:\", remove_mentions(sample_text))\n","print(\"After hashtag removal:\", remove_hashtags(sample_text))\n","print(\"After number removal:\", remove_numbers(sample_text))\n","print(\"After punctuation removal:\", remove_punctuation(sample_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FPlj-13_AjZF","executionInfo":{"status":"ok","timestamp":1766238352115,"user_tz":-180,"elapsed":23,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"db97c3e7-9b02-4a6b-844a-c9e7a0897424"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Cleaning functions created!\n","\n","Original: @john Hey! Check this out https://example.com #awesome 123\n","After URL removal: @john Hey! Check this out  #awesome 123\n","After mention removal:  Hey! Check this out https://example.com #awesome 123\n","After hashtag removal: @john Hey! Check this out https://example.com awesome 123\n","After number removal: @john Hey! Check this out https://example.com #awesome \n","After punctuation removal: john Hey Check this out httpsexamplecom awesome 123\n"]}]},{"cell_type":"code","source":["# Apply full cleaning pipeline\n","def clean_text(text):\n","    \"\"\"Complete text cleaning pipeline\"\"\"\n","    text = str(text)  # Ensure string\n","    text = to_lowercase(text)\n","    text = remove_urls(text)\n","    text = remove_mentions(text)\n","    text = remove_hashtags(text)\n","    text = remove_numbers(text)\n","    text = remove_punctuation(text)\n","    text = remove_extra_spaces(text)\n","    return text\n","\n","print(\"Cleaning all tweets...\")\n","print(\"This may take 1-2 minutes...\\n\")\n","\n","# Apply cleaning\n","df['text_cleaned'] = df['text'].apply(clean_text)\n","\n","print(\"‚úÖ Text cleaning complete!\")\n","\n","# Compare before and after\n","print(\"\\n\" + \"=\"*70)\n","print(\"BEFORE vs AFTER CLEANING\")\n","print(\"=\"*70)\n","for i in range(5):\n","    print(f\"\\nTweet {i+1}:\")\n","    print(f\"Before: {df['text'].iloc[i]}\")\n","    print(f\"After:  {df['text_cleaned'].iloc[i]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_wZFyJSAnHZ","executionInfo":{"status":"ok","timestamp":1766238362948,"user_tz":-180,"elapsed":1951,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"79d10e68-e8d8-4884-a49c-c1780eb0b525"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaning all tweets...\n","This may take 1-2 minutes...\n","\n","‚úÖ Text cleaning complete!\n","\n","======================================================================\n","BEFORE vs AFTER CLEANING\n","======================================================================\n","\n","Tweet 1:\n","Before: @stargazer60 that's awesome \n","After:  thats awesome\n","\n","Tweet 2:\n","Before: @cunningstunts22  yes... i am!  \n","After:  yes i am\n","\n","Tweet 3:\n","Before: N my bed aaallll alone \n","After:  n my bed aaallll alone\n","\n","Tweet 4:\n","Before: @Thorney88 re guinea fowl ive heard that lots of people dislike it!  - THEY SAY ITS FOWL!!!  lol \n","After:  re guinea fowl ive heard that lots of people dislike it they say its fowl lol\n","\n","Tweet 5:\n","Before: def going to the movies tonite \n","After:  def going to the movies tonite\n"]}]},{"cell_type":"code","source":["# Apply full cleaning pipeline\n","def clean_text(text):\n","    \"\"\"Complete text cleaning pipeline\"\"\"\n","    text = str(text)  # Ensure string\n","    text = to_lowercase(text)\n","    text = remove_urls(text)\n","    text = remove_mentions(text)\n","    text = remove_hashtags(text)\n","    text = remove_numbers(text)\n","    text = remove_punctuation(text)\n","    text = remove_extra_spaces(text)\n","    return text\n","\n","print(\"Cleaning all tweets...\")\n","print(\"This may take 1-2 minutes...\\n\")\n","\n","# Apply cleaning\n","df['text_cleaned'] = df['text'].apply(clean_text)\n","\n","print(\"‚úÖ Text cleaning complete!\")\n","\n","# Compare before and after\n","print(\"\\n\" + \"=\"*70)\n","print(\"BEFORE vs AFTER CLEANING\")\n","print(\"=\"*70)\n","for i in range(5):\n","    print(f\"\\nTweet {i+1}:\")\n","    print(f\"Before: {df['text'].iloc[i]}\")\n","    print(f\"After:  {df['text_cleaned'].iloc[i]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gm9GIMriApRq","executionInfo":{"status":"ok","timestamp":1766238425347,"user_tz":-180,"elapsed":3229,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"c76a9e5b-7247-4361-a5f8-86fbf7f97495"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaning all tweets...\n","This may take 1-2 minutes...\n","\n","‚úÖ Text cleaning complete!\n","\n","======================================================================\n","BEFORE vs AFTER CLEANING\n","======================================================================\n","\n","Tweet 1:\n","Before: @stargazer60 that's awesome \n","After:  thats awesome\n","\n","Tweet 2:\n","Before: @cunningstunts22  yes... i am!  \n","After:  yes i am\n","\n","Tweet 3:\n","Before: N my bed aaallll alone \n","After:  n my bed aaallll alone\n","\n","Tweet 4:\n","Before: @Thorney88 re guinea fowl ive heard that lots of people dislike it!  - THEY SAY ITS FOWL!!!  lol \n","After:  re guinea fowl ive heard that lots of people dislike it they say its fowl lol\n","\n","Tweet 5:\n","Before: def going to the movies tonite \n","After:  def going to the movies tonite\n"]}]},{"cell_type":"code","source":["# Get English stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Option to keep some important words for sentiment\n","# Remove negation words from stopwords (they're important for sentiment!)\n","negation_words = {'not', 'no', 'nor', 'neither', 'never', 'none', 'nobody', 'nothing', 'nowhere', 'hardly', 'barely', 'scarcely'}\n","stop_words = stop_words - negation_words\n","\n","print(f\"Total stopwords: {len(stop_words)}\")\n","print(f\"Sample stopwords: {list(stop_words)[:20]}\")\n","print(f\"\\nKept negation words for sentiment: {negation_words}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53tc0387A4NP","executionInfo":{"status":"ok","timestamp":1766238445509,"user_tz":-180,"elapsed":29,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"34f43fda-8127-492b-8bca-1297fcb679d3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Total stopwords: 195\n","Sample stopwords: ['re', 'couldn', 'have', 'we', 'as', \"he's\", 'his', 'for', 'she', \"you've\", 'theirs', 'you', \"she'd\", 'most', 'under', \"isn't\", 'this', 'where', 'being', \"you'll\"]\n","\n","Kept negation words for sentiment: {'not', 'none', 'hardly', 'nor', 'barely', 'nobody', 'neither', 'no', 'nothing', 'never', 'scarcely', 'nowhere'}\n"]}]},{"cell_type":"code","source":["def remove_stopwords(text):\n","    \"\"\"Remove stopwords while keeping negations\"\"\"\n","    words = text.split()\n","    filtered_words = [word for word in words if word not in stop_words]\n","    return ' '.join(filtered_words)\n","\n","print(\"Removing stopwords...\")\n","\n","df['text_no_stopwords'] = df['text_cleaned'].apply(remove_stopwords)\n","\n","print(\"‚úÖ Stopwords removed!\")\n","\n","# Compare\n","print(\"\\n\" + \"=\"*70)\n","print(\"WITH vs WITHOUT STOPWORDS\")\n","print(\"=\"*70)\n","for i in range(3):\n","    print(f\"\\nTweet {i+1}:\")\n","    print(f\"With stopwords: {df['text_cleaned'].iloc[i]}\")\n","    print(f\"Without:        {df['text_no_stopwords'].iloc[i]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L_LlvpRxA96b","executionInfo":{"status":"ok","timestamp":1766238475840,"user_tz":-180,"elapsed":914,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"3916bb28-c920-444e-e70d-213a468955b1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Removing stopwords...\n","‚úÖ Stopwords removed!\n","\n","======================================================================\n","WITH vs WITHOUT STOPWORDS\n","======================================================================\n","\n","Tweet 1:\n","With stopwords: thats awesome\n","Without:        thats awesome\n","\n","Tweet 2:\n","With stopwords: yes i am\n","Without:        yes\n","\n","Tweet 3:\n","With stopwords: n my bed aaallll alone\n","Without:        n bed aaallll alone\n"]}]},{"cell_type":"code","source":["# Initialize lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","def lemmatize_text(text):\n","    \"\"\"Lemmatize words to their root form\"\"\"\n","    words = text.split()\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","    return ' '.join(lemmatized_words)\n","\n","print(\"Lemmatizing text...\")\n","print(\"This may take 2-3 minutes...\\n\")\n","\n","df['text_lemmatized'] = df['text_no_stopwords'].apply(lemmatize_text)\n","\n","print(\"‚úÖ Lemmatization complete!\")\n","\n","# Compare\n","print(\"\\n\" + \"=\"*70)\n","print(\"BEFORE vs AFTER LEMMATIZATION\")\n","print(\"=\"*70)\n","for i in range(3):\n","    print(f\"\\nTweet {i+1}:\")\n","    print(f\"Before: {df['text_no_stopwords'].iloc[i]}\")\n","    print(f\"After:  {df['text_lemmatized'].iloc[i]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4-xYqMIBFF8","executionInfo":{"status":"ok","timestamp":1766238498017,"user_tz":-180,"elapsed":8081,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"f4785b92-5111-48d6-8ead-b12cb6ad7374"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemmatizing text...\n","This may take 2-3 minutes...\n","\n","‚úÖ Lemmatization complete!\n","\n","======================================================================\n","BEFORE vs AFTER LEMMATIZATION\n","======================================================================\n","\n","Tweet 1:\n","Before: thats awesome\n","After:  thats awesome\n","\n","Tweet 2:\n","Before: yes\n","After:  yes\n","\n","Tweet 3:\n","Before: n bed aaallll alone\n","After:  n bed aaallll alone\n"]}]},{"cell_type":"code","source":["# Calculate statistics\n","df['cleaned_length'] = df['text_cleaned'].apply(len)\n","df['cleaned_word_count'] = df['text_cleaned'].apply(lambda x: len(x.split()))\n","df['lemmatized_word_count'] = df['text_lemmatized'].apply(lambda x: len(x.split()))\n","\n","print(\"=\"*70)\n","print(\"TEXT PROCESSING STATISTICS\")\n","print(\"=\"*70)\n","\n","stats = pd.DataFrame({\n","    'Stage': ['Original', 'Cleaned', 'After Stopwords', 'Lemmatized'],\n","    'Avg Length (chars)': [\n","        df['text'].apply(len).mean(),\n","        df['text_cleaned'].apply(len).mean(),\n","        df['text_no_stopwords'].apply(len).mean(),\n","        df['text_lemmatized'].apply(len).mean()\n","    ],\n","    'Avg Word Count': [\n","        df['text'].apply(lambda x: len(str(x).split())).mean(),\n","        df['cleaned_word_count'].mean(),\n","        df['text_no_stopwords'].apply(lambda x: len(x.split())).mean(),\n","        df['lemmatized_word_count'].mean()\n","    ]\n","})\n","\n","print(stats.to_string(index=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k_OU5HCHBIwF","executionInfo":{"status":"ok","timestamp":1766238523218,"user_tz":-180,"elapsed":1544,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"8a544564-45af-45b3-8068-63271587b470"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","TEXT PROCESSING STATISTICS\n","======================================================================\n","          Stage  Avg Length (chars)  Avg Word Count\n","       Original           74.422547       13.236491\n","        Cleaned           62.195708       12.441046\n","After Stopwords           43.601759        7.305998\n","     Lemmatized           43.061455        7.305998\n"]}]},{"cell_type":"code","source":["# Visualize cleaning impact\n","fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n","\n","# 1. Word count comparison\n","stages = ['Original', 'Cleaned', 'No Stopwords', 'Lemmatized']\n","word_counts = [\n","    df['text'].apply(lambda x: len(str(x).split())).mean(),\n","    df['cleaned_word_count'].mean(),\n","    df['text_no_stopwords'].apply(lambda x: len(x.split())).mean(),\n","    df['lemmatized_word_count'].mean()\n","]\n","\n","axes[0, 0].bar(stages, word_counts, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'], alpha=0.8)\n","axes[0, 0].set_ylabel('Average Word Count', fontsize=12)\n","axes[0, 0].set_title('Impact of Preprocessing on Word Count', fontsize=14, fontweight='bold')\n","axes[0, 0].grid(axis='y', alpha=0.3)\n","for i, v in enumerate(word_counts):\n","    axes[0, 0].text(i, v + 0.5, f'{v:.1f}', ha='center', fontweight='bold')\n","\n","# 2. Distribution of word counts (cleaned)\n","axes[0, 1].hist(df[df['sentiment']==0]['cleaned_word_count'],\n","                bins=30, alpha=0.6, label='Negative', color='#e74c3c')\n","axes[0, 1].hist(df[df['sentiment']==1]['cleaned_word_count'],\n","                bins=30, alpha=0.6, label='Positive', color='#2ecc71')\n","axes[0, 1].set_xlabel('Word Count', fontsize=12)\n","axes[0, 1].set_ylabel('Frequency', fontsize=12)\n","axes[0, 1].set_title('Word Count Distribution After Cleaning', fontsize=14, fontweight='bold')\n","axes[0, 1].legend()\n","axes[0, 1].grid(alpha=0.3)\n","\n","# 3. Text length comparison\n","axes[1, 0].boxplot([\n","    df['text'].apply(len),\n","    df['text_cleaned'].apply(len),\n","    df['text_no_stopwords'].apply(len),\n","    df['text_lemmatized'].apply(len)\n","], labels=stages)\n","axes[1, 0].set_ylabel('Character Length', fontsize=12)\n","axes[1, 0].set_title('Text Length at Different Stages', fontsize=14, fontweight='bold')\n","axes[1, 0].grid(axis='y', alpha=0.3)\n","\n","# 4. Sample comparison table\n","axes[1, 1].axis('off')\n","comparison_text = \"\"\"\n","PREPROCESSING PIPELINE SUMMARY\n","\n","Stages Applied:\n","1. Lowercase conversion\n","2. URL removal\n","3. @mention removal\n","4. Hashtag symbol removal\n","5. Number removal\n","6. Punctuation removal\n","7. Extra space removal\n","8. Stopword removal (kept negations)\n","9. Lemmatization\n","\n","Results:\n","- Average word reduction: {:.1f}%\n","- Cleaned tweets: {:,}\n","- Ready for feature extraction\n","\n","Next Steps (Person 2):\n","‚Üí TF-IDF vectorization\n","‚Üí Feature engineering\n","\"\"\".format(\n","    (1 - df['lemmatized_word_count'].mean() / df['text'].apply(lambda x: len(str(x).split())).mean()) * 100,\n","    len(df)\n",")\n","\n","axes[1, 1].text(0.1, 0.5, comparison_text,\n","                fontsize=11, family='monospace',\n","                verticalalignment='center',\n","                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n","\n","plt.tight_layout()\n","plt.savefig('results/figures/07_person1_text_cleaning.png', dpi=300, bbox_inches='tight')\n","plt.show()\n","\n","print(\"‚úÖ Saved: results/figures/07_person1_text_cleaning.png\")"],"metadata":{"id":"o3bNK-hIBQf7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save preprocessed data for Person 2 and Person 3\n","output_file = 'data/processed/tweets_preprocessed_person1.csv'\n","\n","# Select relevant columns\n","df_output = df[['sentiment', 'text', 'text_cleaned', 'text_no_stopwords',\n","                'text_lemmatized', 'cleaned_word_count', 'lemmatized_word_count']]\n","\n","df_output.to_csv(output_file, index=False)\n","\n","file_size = os.path.getsize(output_file) / (1024**2)\n","\n","print(f\"‚úÖ Preprocessed data saved!\")\n","print(f\"üìÅ Location: {output_file}\")\n","print(f\"üíæ Size: {file_size:.2f} MB\")\n","print(f\"üìä Rows: {len(df_output):,}\")\n","print(f\"üìã Columns: {list(df_output.columns)}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"‚úÖ PERSON 1 TASK COMPLETE!\")\n","print(\"=\"*70)\n","print(\"Deliverables:\")\n","print(\"  ‚Ä¢ Cleaned and preprocessed text\")\n","print(\"  ‚Ä¢ Removed URLs, mentions, numbers, punctuation\")\n","print(\"  ‚Ä¢ Removed stopwords (kept negations)\")\n","print(\"  ‚Ä¢ Lemmatized text\")\n","print(\"  ‚Ä¢ Saved processed data for team\")\n","print(\"  ‚Ä¢ Created visualization\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMxPZJeJBnWn","executionInfo":{"status":"ok","timestamp":1766238677123,"user_tz":-180,"elapsed":3646,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"c4a4cb4e-d23c-437d-97cf-d4e80552a08e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Preprocessed data saved!\n","üìÅ Location: data/processed/tweets_preprocessed_person1.csv\n","üíæ Size: 22.27 MB\n","üìä Rows: 99,357\n","üìã Columns: ['sentiment', 'text', 'text_cleaned', 'text_no_stopwords', 'text_lemmatized', 'cleaned_word_count', 'lemmatized_word_count']\n","\n","======================================================================\n","‚úÖ PERSON 1 TASK COMPLETE!\n","======================================================================\n","Deliverables:\n","  ‚Ä¢ Cleaned and preprocessed text\n","  ‚Ä¢ Removed URLs, mentions, numbers, punctuation\n","  ‚Ä¢ Removed stopwords (kept negations)\n","  ‚Ä¢ Lemmatized text\n","  ‚Ä¢ Saved processed data for team\n","  ‚Ä¢ Created visualization\n"]}]},{"cell_type":"code","source":["!git config --global user.name \"zeynkash\"  # Replace with your name\n","!git config --global user.email \"030721077@std.izu.edu.tr\""],"metadata":{"id":"r_4sBUhdB1jk","executionInfo":{"status":"ok","timestamp":1766238822421,"user_tz":-180,"elapsed":209,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["%cd /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1pHd4oevCauT","executionInfo":{"status":"ok","timestamp":1766238836577,"user_tz":-180,"elapsed":10,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"05f89fa0-68cb-440d-d84e-e0ddcb230348"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/zeynkash/twitter-sentiment-analysis.git"],"metadata":{"id":"equdmyhcCkNG","executionInfo":{"status":"ok","timestamp":1766238867447,"user_tz":-180,"elapsed":1027,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"8d0883ee-77d4-4b1c-e59c-454f18322c3a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'twitter-sentiment-analysis'...\n","remote: Enumerating objects: 19, done.\u001b[K\n","remote: Counting objects: 100% (19/19), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 19 (delta 0), reused 16 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (19/19), 3.61 MiB | 16.26 MiB/s, done.\n"]}]},{"cell_type":"code","source":["%cd /content/twitter-sentiment-analysis\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n-qjCpEdCfve","executionInfo":{"status":"ok","timestamp":1766238872001,"user_tz":-180,"elapsed":14,"user":{"displayName":"Zeyn Kaskoul","userId":"16532829207240258962"}},"outputId":"25c5e357-8c01-483c-a443-3ed92e287dba"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/twitter-sentiment-analysis\n"]}]},{"cell_type":"code","source":["!cp /content/drive/MyDrive/TwitterSentimentProject/notebooks/02_Phase2_Person1_TextCleaning.ipynb notebooks/\n"],"metadata":{"id":"PXX25KbxCA-m"},"execution_count":null,"outputs":[]}]}